{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Weights Initialization\n",
    "\n",
    "In this notebook, we will create and save locally the weights of a simple Multi-Layer Perceptron (MLP) for the IRIS dataset. \n",
    "\n",
    "The dataset can be downloaded from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/53/iris) or from other sources. It consists of:\n",
    "\n",
    "- **Features**: Four flower characteristics:\n",
    "    - *sepal_length*, *sepal_width*, *petal_length*, *petal_width*\n",
    "    \n",
    "- **Label**: The target variable, which can take one of the following values:\n",
    "    - *setosa*, *versicolor*, *virginica*\n",
    "    \n",
    "- **Dimensions**: The dataset consists of 150 rows, each containing the features and the target label.\n",
    "\n",
    "The MLP is designed to be as simple as possible, as its **forward pass** will later be synthesized using VITIS. The architecture includes:\n",
    "\n",
    "- **2 hidden layers**, each with the **ReLU** activation function\n",
    "- **1 output layer** with 3 neurons (corresponding to the three species)\n",
    "\n",
    "This ensures that the model remains efficient and easy to implement in hardware synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (150, 5) \n",
      "----------------------------------------------------------------\n",
      "Some rows of the dataset:\n",
      "\n",
      "    sepal_length  sepal_width  petal_length  petal_width species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n",
      "2           4.7          3.2           1.3          0.2  setosa\n",
      "3           4.6          3.1           1.5          0.2  setosa\n",
      "4           5.0          3.6           1.4          0.2  setosa\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(4242)\n",
    "\n",
    "df = pd.read_csv('../datasets/iris_dataset/iris_dataset.csv')\n",
    "\n",
    "print(\"Shape of the dataset:\", df.shape, \"\\n----------------------------------------------------------------\")\n",
    "print(\"Some rows of the dataset:\\n\\n\", df.head())\n",
    "\n",
    "# Extract the features and labels\n",
    "X = df.iloc[:, :-1].values  # all columns except the last one are features\n",
    "y = df.iloc[:, -1].values   # the last column is the label\n",
    "\n",
    "# Encode labels (e.g. 'setosa' -> 0)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Save the dataset with encoded labels to a txt file\n",
    "np.savetxt('../datasets/iris_dataset/iris_dataset_encoded.txt', np.c_[X, y], fmt='%f', header='', comments='')\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  # Use 30% for test\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the MLP as a class, inheriting from PyTorch's `nn.Module`. The structure will follow the architecture discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can proceed to train the model. For this task, we will use:\n",
    "\n",
    "- `CrossEntropyLoss` as the **Loss Function**, which is suitable for multi-class classification problems\n",
    "- The `Adam` **Optimizer**, a widely used optimization algorithm\n",
    "\n",
    "We will use $100$ epochs and set the learning rate to $0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.3258, Train Accuracy: 90.48%, Test Accuracy: 88.89%\n",
      "Epoch [20/100], Loss: 0.1060, Train Accuracy: 97.14%, Test Accuracy: 97.78%\n",
      "Epoch [30/100], Loss: 0.1312, Train Accuracy: 95.24%, Test Accuracy: 97.78%\n",
      "Epoch [40/100], Loss: 0.0853, Train Accuracy: 97.14%, Test Accuracy: 100.00%\n",
      "Epoch [50/100], Loss: 0.0675, Train Accuracy: 98.10%, Test Accuracy: 100.00%\n",
      "Epoch [60/100], Loss: 0.0971, Train Accuracy: 96.19%, Test Accuracy: 97.78%\n",
      "Epoch [70/100], Loss: 0.0952, Train Accuracy: 96.19%, Test Accuracy: 97.78%\n",
      "Epoch [80/100], Loss: 0.1020, Train Accuracy: 96.19%, Test Accuracy: 97.78%\n",
      "Epoch [90/100], Loss: 0.0929, Train Accuracy: 96.19%, Test Accuracy: 97.78%\n",
      "Epoch [100/100], Loss: 0.0788, Train Accuracy: 94.29%, Test Accuracy: 97.78%\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "\n",
    "# Check if GPU is available and move the model to GPU if possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Cross-entropy loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) # Adam optimizer with learning rate 0.01\n",
    "\n",
    "# Training loop\n",
    "NUM_EPOCHS = 100\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        _, predicted_train = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted_train == labels).sum().item()\n",
    "\n",
    "    train_accuracy = correct_train / total_train  # Training accuracy\n",
    "\n",
    "    # Evaluate the model after each epoch on test set\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted_test = torch.max(outputs.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted_test == labels).sum().item()\n",
    "\n",
    "    test_accuracy = correct_test / total_test  # Test accuracy\n",
    "\n",
    "    # Print loss and accuracy every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {running_loss/len(train_loader):.4f}, '\n",
    "              f'Train Accuracy: {train_accuracy * 100:.2f}%, Test Accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's extract the weights from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight: (10, 4)\n",
      "fc1.bias: (10,)\n",
      "fc2.weight: (10, 10)\n",
      "fc2.bias: (10,)\n",
      "fc3.weight: (3, 10)\n",
      "fc3.bias: (3,)\n"
     ]
    }
   ],
   "source": [
    "weights = {}\n",
    "for name, param in model.named_parameters():\n",
    "    weights[name] = param.detach().numpy()\n",
    "\n",
    "# Print their shapes to verify the network architecture\n",
    "for name, weight in weights.items():\n",
    "    print(f\"{name}: {weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, save them to a txt file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('./weights.txt', 'w') as f:\n",
    "    for name, weight in weights.items():\n",
    "        f.write(f\"{name}\\n\")\n",
    "        np.savetxt(f, weight, fmt='%f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SEAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
