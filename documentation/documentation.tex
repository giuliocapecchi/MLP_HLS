\documentclass{article}
% Useful packages
\usepackage{amsmath} % Required for advanced math typesetting
\usepackage{graphicx} % Required for inserting images
\usepackage{xcolor} % Required for color definitions
\usepackage{listings}
\usepackage{tcolorbox} % Required for creating boxes
\usepackage{float} % Required for controlling the position of images


% Code settings
\lstset{
    language=C,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{orange},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    stepnumber=1
}

\setlength{\parindent}{0pt}

\title{SEAI\_2024\_R12}
\author{Giulio Capecchi, Jacopo Niccolai}
\date{December 2024}


\begin{document}

\maketitle

\vspace{3cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{assets/unipi.png}
    \label{fig:unipi_logo}
\end{figure}

\newpage

\tableofcontents

\newpage

\section{Introduction}
This project focuses on the synthesis of the forward pass for three types of neural network architectures: a \textbf{Multilayer Perceptron (MLP)}, a \textbf{Convolutional Neural Network (ConvNet)}, and a \textbf{Transformer}, implemented on an FPGA.  
To achieve this, the network parameters were first obtained using Python and the \textit{PyTorch} library. These parameters were subsequently hardcoded into \texttt{C} code, enabling the hardware synthesis process.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{assets/vitis-hls.png}
    \caption{Xilinx Vitis HLS}
    \label{fig:vitis_hls}
\end{figure}


\section{Project Description}
\subsection{Workflow Overview}
The neural networks were constructed and trained using the \textit{PyTorch} library. Once trained, the weights and biases were exported to be hardcoded into the corresponding \texttt{C} implementation. The \texttt{C} code was specifically designed to be compatible with FPGA synthesis tools, such as Vitis HLS, ensuring efficient hardware synthesis.


\subsection{C Implementation}
The \texttt{C} code developed includes the forward pass for:
\begin{itemize}
    \item \textbf{MLP}: implementation of propagation through dense layers.
    \item \textbf{ConvNet}: handling of convolution and pooling operations.
    \item \textbf{Transformer}: managing complex operations like attention.
\end{itemize}

\#TODO : parlare delle direttive HLS
/*
HLS INLINE
forza l'inserimento completo del corpo di una funzione o di un ciclo direttamente nel punto in cui è chiamato, eliminando l'overhead associato alla chiamata di funzione o all'allocazione di risorse hardware separate.
Riduce la latenza perché elimina il ritardo di chiamata della funzione. Può aumentare l'area hardware utilizzata, perché la logica della funzione viene replicata ovunque venga chiamata.
Tipicamente per funzioni semplici o molto usate, per ridurre la latenza.

HLS PIPELINE
La direttiva HLS PIPELINE suddivide un ciclo o una funzione in più fasi (pipeline), permettendo l'esecuzione di più iterazioni o operazioni simultaneamente, aumentando il throughput del design.
Permette l'elaborazione di nuove iterazioni a ogni ciclo di clock (o a intervalli specificati, chiamati Initiation Interval - II).
Aumenta la velocità del sistema a scapito del consumo di risorse hardware.
Opzioni:
II=N (specifica l'intervallo di avvio delle iterazioni, ad esempio, 1 ciclo di clock tra una e l'altra).
rewind (riavvia automaticamente il ciclo alla fine).
Uso tipico
Nei loop che processano grandi quantità di dati.
Per aumentare il throughput in operazioni ripetitive.
*/

The network parameters (weights and biases) were directly integrated into the code in a hardcoded manner.

\section{Code Architecture}
\subsection{C File Structure}
The forward pass is implemented using a sequence of functions for each layer type:
\begin{itemize}
    \item Activation functions (\texttt{relu}, \texttt{softmax}, etc.).
    \item Functions for convolution and pooling operations.
    \item Functions for attention mechanisms in Transformers.
\end{itemize}


\subsection{Hardware Synthesis}
The code was designed to be compatible with tools such as Vitis HLS, leveraging specific pragmas to optimize the implementation.

\section{Multi-Layer Perceptron}
Let's analyze the implementation of the forward pass for a Multi-Layer Perceptron. The forward pass for an MLP consists of propagating the input through a series of dense layers, each followed by an activation function.
The code for it can be found inside the \texttt{PyTorch} folder, that contains the notebooks used to train the models and export their parameters.

\subsection{Dataset}
The MLP was trained using the well-known \textit{Iris} dataset, which contains 150 samples of iris flowers, each with four features and a class label (the last value of each row).
There is a total of three classes: \textit{setosa}, \textit{versicolor}, and \textit{virginica}.
The dataset was split into training and test sets, with 80\% of the samples used for training and 20\% for testing.

An example of the dataset is shown below:

\begin{tcolorbox}[colback=gray!5, colframe=black, rounded corners, boxrule=0.1mm]
\textbf{\textcolor{blue}{sepal\_length,sepal\_width,petal\_length,petal\_width,species}} \\
5.1,3.5,1.4,0.2,setosa \\
5.7,2.8,4.5,1.3,versicolor \\
6.1,2.6,5.6,1.4,virginica \\
...
\end{tcolorbox}

The dataset's labels were encoded as integers using the Scikit-learn \texttt{LabelEncoder} class, which maps each class to a unique integer value.
To facilitate its further usage inside the \texttt{C} implementation, this encoded version of the dataset was saved to a txt file, \textit{iris\_dataset\_encoded.txt}.

\subsection{PyTorch Model}
\subsubsection{Model Architecture}
The architecture of the MLP model consists of three fully connected (dense) layers.
The input layer has 4 neurons corresponding to the 4 features of the Iris dataset. 
The first and second hidden layers each have 10 neurons, and the output layer has 3 neurons corresponding to the 3 classes of the Iris dataset.
The chosen activation function is the ReLU function, which is applied after each dense layer except the output layer.
The forward pass of the model involves applying the ReLU activation function after the first and second layers.
\\ReLU is defined as:
\begin{center}
$\operatorname{ReLU}(x) = \max(0, x)$
\end{center}
The model was defined as follows, using the \textit{PyTorch} library:
\begin{lstlisting}[language=Python]
# Define the MLP model
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(4, 10)
        self.fc2 = nn.Linear(10, 10)
        self.fc3 = nn.Linear(10, 3)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x
\end{lstlisting}


\subsubsection{Model Training}
For the training phase, we first defined the model, loss function, and optimizer.
We utilized the \textit{CrossEntropyLoss} loss function, which is commonly used for multi-class classification problems, and the \textit{Adam} optimizer, which is an adaptive learning rate optimization algorithm.

\begin{lstlisting}[language=Python]
model = MLP()

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)
\end{lstlisting}

The following code snippet demonstrates the trainign process using \textit{PyTorch}. The model is trained for 100 epochs.

\begin{lstlisting}[language=Python]
# Training loop
NUM_EPOCHS = 100
for epoch in range(NUM_EPOCHS):
    model.train()
    running_loss = 0.0
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    # Evaluate the model after each epoch
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1) # Get the class index with the highest probability
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = correct / total

    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy * 100:.2f}%')
\end{lstlisting}

The results of the training process are reported in the table below:
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Epoch} & \textbf{Loss} & \textbf{Train Accuracy} & \textbf{Test Accuracy} \\
        \hline
        10 & 0.3258 & 90.48\% & 88.89\% \\
        20 & 0.1060 & 97.14\% & 97.78\% \\
        30 & 0.1312 & 95.24\% & 97.78\% \\
        40 & 0.0853 & 97.14\% & 100.00\% \\
        50 & 0.0675 & 98.10\% & 100.00\% \\
        60 & 0.0971 & 96.19\% & 97.78\% \\
        70 & 0.0952 & 96.19\% & 97.78\% \\
        80 & 0.1020 & 96.19\% & 97.78\% \\
        90 & 0.0929 & 96.19\% & 97.78\% \\
        100 & 0.0788 & 94.29\% & 97.78\% \\
        \hline
    \end{tabular}
    \caption{\centering Training loss, train accuracy, and test accuracy of the MLP over 100 epochs.}
    \label{tab:mlp-training}
\end{table}


The training process shows that the model is able to achieve a high level of accuracy on both the training and test sets: this is to be expected given the simplicity of the Iris dataset and the effectiveness of the MLP architecture in solving such problems.
Given the relatively small dataset and the fact that the model achieves near-perfect accuracy on the test set, we can conclude that the model is generalizing well.

\subsubsection{Exporting Parameters}
The trained model's parameters were exported to be hardcoded into the \texttt{C} implementation. The weights and biases of each layer were extracted and saved in a txt file, \textit{mlp\_weights.txt}, as shown below:
\begin{lstlisting}[language=Python]
import numpy as np

weights = {}
for name, param in model.named_parameters():
    weights[name] = param.detach().numpy()

# Print their shapes to verify the network architecture
for name, weight in weights.items():
    print(f"{name}: {weight.shape}")

with open('./mlp_weights.txt', 'w') as f:
for name, weight in weights.items():
    f.write(f"{name}\n")
    np.savetxt(f, weight, fmt='%f')
\end{lstlisting}

\subsection{C Implementation}

The implementation for Vitis HLS of the MLP was produced with three files:
\begin{itemize}
    \item \texttt{mlp.c}, which contains the forward pass function, the activation function, and the definition of the MLP structure.
    \item \texttt{mlp.h}, which contains the definition of the MLP structure and the forward pass function prototype.
    \item \texttt{testbench.c}, which reads the \textit{iris\_dataset\_encoded.txt}  and contains the main function to test the forward pass function.
\end{itemize}

\subsubsection{MLP Structure}
The MLP structure was defined as follows (inside \texttt{mlp.h}):
\begin{lstlisting}
typedef struct {
    float weights[MAX_NEURONS][MAX_NEURONS]; // matrix
    float biases[MAX_NEURONS];   // biases of the layer
    float output[MAX_NEURONS];   // output of the layer
} Layer;

typedef struct {
    int num_layers;              // number of layers
    Layer layers[MAX_LAYERS];    // layers of the MLP (array of layers)
} MLP;
\end{lstlisting}

\texttt{MAX\_NEURONS} and \texttt{MAX\_LAYERS} are defined as 100 and 3, respectively.

\subsubsection{Forward Pass}
The forward pass for the MLP is implemented in \texttt{mlp.c} as a sequence of operations applied to each layer of the network. The function processes four input features through three layers, each defined by its respective weights and biases, to produce the predicted class index.
It is defined as follows:
\begin{lstlisting}
int forward(float input0, float input1, float input2, float input3) {
    const int input_sizes[4] = {4, 10, 10, 3}; 
    const int num_layers = 3;

    float current_input[MAX_NEURONS];
    float next_input[MAX_NEURONS];

    current_input[0] = input0;
    current_input[1] = input1;
    current_input[2] = input2;
    current_input[3] = input3;

    for (int i = 0; i < num_layers; i++) {
        #pragma HLS UNROLL
        Layer *layer = &mlp.layers[i];
        for (int j = 0; j < input_sizes[i + 1]; j++) {
            float sum = layer->biases[j];
            
            for (int k = 0; k < input_sizes[i]; k++) {
                sum += layer->weights[j][k] * current_input[k];
            }
            next_input[j] = reLu(sum);
        }

        for (int j = 0; j < input_sizes[i + 1]; j++) {
            current_input[j] = next_input[j];
        }
    }

    int max_index = 0;
    float max = current_input[0];
    for (int i = 1; i < NUM_CLASSES; i++) {
        #pragma HLS UNROLL
        if (current_input[i] > max) {
            max = current_input[i];
            max_index = i;
        }
    }
    return max_index;
}
\end{lstlisting}

In this implementation, all weights and biases are hardcoded and directly integrated into the MLP definition at the top of \texttt{mlp.c}. These values are exported from the PyTorch model and inserted manually during the setup phase.
\\A notable feature of this implementation is the use of the \texttt{\#pragma HLS UNROLL} directive. This instructs the High-Level Synthesis (HLS) tool to unroll loops, which replicates the loop body multiple times. This mechanism significantly enhances the throughput of the design by enabling parallel execution of loop iterations: as a result, the forward pass should achieve higher performances, making it suitable for FPGA-based acceleration.

\subsubsection{Testbench}
The testbench function reads the encoded Iris dataset file and applies the forward pass function to each sample.
The file is read line-by-line by using the \texttt{fscanf} function, and the four features are passed to the forward pass function.
The predicted class index is then compared with the actual class index to calculate the accuracy of the model.
\\\\Below the code for the testbench function:
\begin{lstlisting}
int read_data_from_file(const char *path, int num_features, int label_size, float input_data[MAX_SAMPLES][MAX_FEATURES], float true_value[MAX_SAMPLES]) {
    FILE *file = fopen(path, "r");
    if (!file) {
        perror("Failed to open file");
        return -1;
    }

    int sample_count = 0;
    while (fscanf(file, "%f", &input_data[sample_count][0]) != EOF) {
        for (int i = 1; i < num_features; i++) {
            fscanf(file, "%f", &input_data[sample_count][i]);
        }
        for (int j = 0; j < label_size; j++) {
            fscanf(file, "%f", &true_value[sample_count]);
        }
        sample_count++;
        if (sample_count >= MAX_SAMPLES) {
            break;
        }
    }

    fclose(file);
    return sample_count;
}

int main() {
float input_data[MAX_SAMPLES][MAX_FEATURES];
float true_value[MAX_SAMPLES];

// Read data from file
const char *path = "./datasets/iris_dataset/iris_dataset_encoded.txt";
int sample_count = read_data_from_file(path, MAX_FEATURES, 1, input_data, true_value);

// call the forward function and calculate the accuracy
int correct_predictions = 0;
for (int i = 0; i < sample_count; i++) {
    int prediction = forward(input_data[i][0], input_data[i][1], input_data[i][2], input_data[i][3]);
    if (prediction == true_value[i]) {
        correct_predictions++;
    }else{
        printf("Prediction: %d, True value: %f for input: %f %f %f %f\n", prediction, true_value[i], input_data[i][0], input_data[i][1], input_data[i][2], input_data[i][3]);
    }
}
float accuracy = (float)correct_predictions / sample_count * 100.0;
printf("Accuracy: %.2f%%\n", accuracy);
}
\end{lstlisting}

The testbench is used to test that everything is working correctly and to evaluate the accuracy of the model on the dataset. The accuracy obtained should be similar to the one achieved during the training phase in \textit{PyTorch}, confirming that the forward pass function is correctly implemented in \texttt{C}.
We always obtained an accuracy of 98\% with it, consistent with the results obtained with \textit{PyTorch}.

\subsection{Results}
The results obtained using the Vitis Unified IDE confirm the successful synthesis and implementation of the MLP forward pass on the FPGA. The development process in Vitis offers several stages where detailed reports are generated, providing valuable insights into the design's functionality and performance. The used device for this section was from the Product family \textbf{zynq}, and the Target device used was the \textbf{xc7z007s-clg225-2}.

\subsubsection{Stages of Development}
These stages include:

\begin{itemize}
    \item \textbf{C Simulation:} This initial step ensures the functional correctness of the high-level C implementation. During this phase, the input data is processed entirely in software, and the generated reports confirm that the output matches the expected results, validating the logic before hardware synthesis.

    \item \textbf{C Synthesis:} In this phase, the high-level C code is converted into a hardware description optimized for the target FPGA. The synthesis report provides crucial details, such as estimated resource utilization (LUTs, DSPs, BRAMs), latency, and initiation intervals. These metrics help identify potential bottlenecks and guide optimization efforts.

    \item \textbf{C/RTL Simulation:} This step bridges the gap between high-level and low-level design by validating the synthesized hardware description against the functional requirements. This stage is particularly important as it ensures consistency between the high-level model and the Register Transfer Level (RTL) implementation. The reports include timing diagrams, functional waveforms, and a comparison of C simulation outputs with RTL simulation outputs to confirm correctness.

    \item \textbf{Packaging:} After verifying the synthesized hardware, the design is packaged into an IP (Intellectual Property) core. The packaging reports detail the generated IP core's properties, ensuring that it adheres to the FPGA's integration requirements and is ready for system-level implementation.

    \item \textbf{Implementation:} In the final stage, the IP core is placed and routed on the FPGA. Implementation reports include metrics such as timing analysis, power estimates, and resource utilization on the physical FPGA fabric. These reports confirm that the design meets the FPGA's constraints, such as timing closure and power consumption.

\end{itemize}
By consulting these reports, the development process is highly transparent: each step ensures the correctness, performance, and compliance with the FPGA's requirements, resulting in an efficient and reliable implementation of the top-function, in this case the forward pass.
\\\\These steps are valid also for the further architectures, so we will not repeat them in the following sections, but just present them.

\subsubsection{Performance Metrics}
Let's go into detail about the performance metrics obtained during the synthesis of the MLP forward pass on the FPGA.

\paragraph{C-Simulation}  
C-simulation provides preliminary performance metrics, focusing on the steady-state execution of the design. These estimates, including the Transaction Interval (TI), highlight potential bottlenecks and optimization areas but may be overly optimistic unless the code is made canonical. This stage serves as an initial evaluation, guiding further refinement and more accurate analysis during synthesis and co-simulation.  
Here, we could mainly observe the correctness of the code (given by the expected output in the terminal), but we also noticed that there are some dependencies in the code: indeed, we obtained the following guidance message \texttt{SIM 211-201A cyclic dependence prevents further acceleration of this process. This generally requires some algorithmic changes to improve}. However, we still have to remember that this is the pre-synthesis phase, so we can't expect the best performance metrics yet. As we will see, results will be good in the following stages. 


\paragraph{C-Synthesis}
Here, we can see the results of the synthesis of the MLP forward pass on the FPGA. The table below shows the \textit{Estmated Quality of results}, which is the first metric presented in the report:

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{TARGET} & \textbf{ESTIMATED} & \textbf{UNCERTAINTY} \\
        \hline
        10.00 ns & 6.329 ns & 2.70 ns \\
        \hline
    \end{tabular}
    \caption{\centering Estimated Quality of Results for MLP Forward Pass}
    \label{tab:mlp-quality}
\end{table}

As we can see from the table, the estimated latency is 6.329 ns, with an uncertainty of 2.70 ns. This metric provides an initial indication of the design's performance, with lower values indicating faster execution. The uncertainty value represents the range within which the actual latency is expected to fall, providing a margin of error for the estimation. This falls within the expected range for the MLP forward pass, indicating that the design should be good for efficient execution.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{./assets/MLP/c-synthesis-performanceandresourceestimaes.png}
    \caption{Perfomance and Resource Estimates in the C-Synthesis Report}
    \label{fig:c-synthesis-performance-resources}
\end{figure}

TODO : questa parte qui va controllata meglio
\\\\From the above image, we observe that the \texttt{forward} module exhibits an overall estimated latency of 216 cycles, corresponding to an execution time of $2,160\ \mathrm{\mu s}$ under the target clock conditions. The report highlights that the main module was not fully pipelined, with an \textit{interval} of 217 cycles. This indicates that the module optimization could benefit from increased parallelization, especially within the internal loops.
Examining the subsections of the \texttt{forward} module, we notice that several internal loops (\texttt{forward\_Pipeline\_VITIS\_LOOP}) were optimized using \textit{auto unrolling} and \textit{pipelining} techniques, with latencies ranging from 5 ns to 690 ns. However, the main loop of the module shows a relatively high latency (690 ns), indicating a potential bottleneck. Additionally, no usage of \texttt{URAM} is reported, while the DSP resources utilized amount to 50, with a significant number of Flip-Flops (\texttt{FF}) and Look-Up Tables (\texttt{LUT}) employed, totaling 7417 and 9689, respectively.
These results suggest that there is room for improvement through increased parallelization and better utilization of available hardware resources, particularly by leveraging internal memory (\texttt{URAM}) and reducing the execution interval (\textit{interval}) for critical loops.
\\\\We can also check from the report that, as expected the hardware interface corresponds to the input and output of the forward pass function, and that the utilized pragma syntax is correct and corresponds to the one we used in the code.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{./assets/MLP/c-synthesis-HWinterfaces.png}
        \caption{Hardware interfaces}
        \label{fig:c-synthesis-hw-interfaces}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{./assets/MLP/c-synthesis-validpragmasyntax.png}
        \caption{Pragma syntax}
        \label{fig:c-synthesis-pragma-syntax}
    \end{minipage}
\end{figure}


\paragraph{C/RTL Simulation}

C-RTL cosimulation is a verification process that ensures the functional equivalence between the high-level C/C++ design and the synthesized Register-Transfer Level (RTL) code. This step is critical as it confirms that the behavior of the RTL implementation matches the original C/C++ description after synthesis. The benefits are multiple:

\begin{itemize}
    \item \textbf{Validation of Functional Correctness}: Verifies that the generated RTL implementation functions identically to the original high-level design for the same inputs.
    \item \textbf{Timing and Latency Estimates}: Provides insights into the actual timing behavior of the synthesized RTL.
    \item \textbf{Resource Utilization Check}: Highlights any discrepancies between resource usage reported during synthesis and actual utilization in hardware.
\end{itemize}

\subparagraph{How It Works}
\begin{enumerate}
    \item \textbf{Input Stimuli}: A testbench written in C/C++ is used to provide input data to both the high-level C/C++ design and the RTL design.
    \item \textbf{Output Comparison}: The outputs from the high-level simulation and the RTL simulation are compared.
    \item \textbf{Reports}: Any mismatches or timing violations are reported for debugging purposes.
\end{enumerate}

\subparagraph{Analysis of Results}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{./assets/MLP/c-rtl-cosimulation.png}
    \caption{C/RTL Cosimulation Report - Performance and resource estimates}
    \label{fig:C-RTL-cosimulation}
\end{figure}


From the above table:

\subparagraph{Initiation Interval (II)}
\begin{itemize}
    \item The initiation interval (II) across all loops is constant at \textbf{205 cycles}, which suggests that the pipeline performance for each loop is uniform and constrained by a similar bottleneck.
\end{itemize}

\subparagraph{Latencies}
\begin{itemize}
    \item \textbf{Average Latency}: Varies for individual loops. For instance:
    \begin{itemize}
        \item The \texttt{forward\_Pipeline\_VITIS\_LOOP\_73\_2} has an average latency of \textbf{37 cycles}.
        \item Other loops like \texttt{forward\_Pipeline\_VITIS\_LOOP\_84\_44} achieve a minimal latency of \textbf{3 cycles}, indicating higher efficiency for certain tasks.
    \end{itemize}
    \item The overall latency for the main \texttt{forward} module is \textbf{204 cycles}, which aligns with the high-level design expectations.
\end{itemize}

\subparagraph{Total Execution Time}
\begin{itemize}
    \item The total execution time for the main \texttt{forward} module is reported as \textbf{30749 ns}, reflecting the aggregated runtime for the design.
\end{itemize}

\subparagraph{Pipeline Observations}
\begin{itemize}
    \item While the loops have been pipelined (as evidenced by consistent II values), the relatively high initiation interval of \textbf{205} could indicate that further optimization is needed to reduce dependencies or resource contention.
\end{itemize}

\subparagraph{Observations and Suggestions}

\begin{itemize}
    \item \textbf{Performance}: The latency and total execution time suggest that the design is functional, but there may be room for improvement in pipelining efficiency. Reducing the II could significantly enhance performance.
    \item \textbf{Resource Utilization}: Resource contention or dependencies among operations might be causing the high II. Reviewing data paths and loop unrolling strategies could help alleviate bottlenecks.
    \item \textbf{C-RTL Cosimulation Validation}: Assuming the results match between the C-level and RTL-level simulations, this confirms the functional correctness of the design and prepares it for downstream FPGA implementation.
\end{itemize}



\section{Convolutional Neural Network (ConvNet)}
The ConvNet forward pass was implemented in a similar manner to the MLP, with the main difference being the convolution and pooling operations. The ConvNet was trained on the famous \textit{MNIST} dataset, which contains 70.000 32x32 black and white images in 10 classes (representing hand-written digits).

\end{document}


